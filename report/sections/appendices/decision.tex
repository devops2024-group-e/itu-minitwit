\section{Decision Log}
\label{appendix:decision-log}

\subsection{Remote Git Repo: Github}
Looking at a remote github tool we have had a bunch of tools that we could use:
\begin{itemize}
    \item GitHub
    \item Azure DevOps
    \item BitBucket
\end{itemize}

What we want from the Remote Git Repo tool is that it of course supports having a remote git repository. But also the following is needed:
\begin{itemize}
    \item Tracking Work. Also having a trace of what work item is associated to what in the code.
    \item Project collaboration. This could be some sort of visualization of workitems and in which state that they are in.
    \item Possibility to create a release(This is a requirement from Helge to track our pace?).
    \item Have CI / CD tools available.
    \item Have Pull Requests or something similar, which can be used to protect pushes directly to main branch
\end{itemize}

As mentioned we have mainly looked at the three tools above. They are all very similar, and provide the same features. Especially GitHub and Azure Devops which is both owned and maintained by Microsoft. All the products above have the above requirements. However, BitBucket and Azure Devops lacks the release creation. They both a binary like form of release, where it is just some sort of an artifact that is pushed to a download link which can be downloaded. This is not what we are looking for in a release. It lacks some documentation of what has actually been included in the release. Which is exactly what GitHub offers.

Also the team has experience working with GitHub since ITU in most of the courses uses that, and have made us use the ITU hosted GitHub Enterprise version. Although we do have some experience with Azure DevOps, then the general feeling is that GitHub is easier to work with, and that will become useful in the weeks ahead that we have a tool that we are comfortable with. So for that reason we choose GitHub as our Remote Repository.

\subsection{Language: C\#}

We decided to use C\# for refactorisation. We did this due to C\# being object-oriented and a stable language. C\# is used by many companies and is therefore known to many. Because C\# is used by many there also exists good documentation easily accessible.
There exist both micro web-frameworks and more advanced frontend frameworks for C\# in case we want to develop the application further.
C\# is also a part of the .NET platform, which allows for language interoperability and access to the Core framework used for web applications.

\subsection{Micro Web Framework: Razor Pages}

Initially, we decided to work with Scriban as the micro web-framework, as opposed to e.g. Razor Pages, because the refactorisation from the original version of Minitwit seemed more direct in Scriban. This is due to the fact that Scriban works with \texttt{.html} templates, like we have in the original version, whereas Razor Pages integrates C\# in HTML, which would lead to more “merging” of code.
Furthermore, Scriban's parser and render are both faster and use less memory than other templates.
After trying to find information on Scriban, the group had trouble figuring out how to set it up properly. Comparing the amount of documentation for Scriban versus Razor Pages, we decided to switch the micro web-framework to Razor Pages, as this would allow us to start working with the material faster.
Razor Pages also has simple context of structure and is flexible to fit with any application. Another reason for choosing Razor Pages is that each page is self-contained, with the view and code organised together. This makes Razor Pages more organised and easier to work with. With the documentation, flexibility and the organisation, Razor Pages felt like a good choice for a micro web-framework.

\subsection{Work Log: Notion}

To start of with, we decided to keep our work log/diary in GitHub, since this automatically provides us with useful information, e.g. dates and authors. Additionally, the work log will follow the code completely, and we can see which code belongs with what log entries. However, we realised after a few weeks that this did not work as intended. The reason was that the action of making a pull request and requesting a review after just a tiny addition to the log seemed too complicated.
As a consequence, we ended up keeping our log in Notion and then moving it to GitHub afterwards, which was double the work and definitely not the intention from the start.
Therefore, we decided to properly move the log to Notion, which also has the advantage that everyone can follow each other's writing in real time and collaborate in writing simultaneously.

\subsection{Database Connection: Entity Framework}

We have chosen Entity Framework (EF) for our database connection as it works well with .NET. EF has the possibility of generating a model from an already existing database and lets us load data using C\# classes. Additionally, EF supports LINQ and works well with SQLite.

\subsection{Server Host: Digital Ocean}

We have chosen Digital Ocean as our cloud hosting service, as it is a cheap cloud hosting service that allows for developers to have quite a lot of control over their server hosting. Furthermore, popular cloud hosting alternatives have a tendency to be more expensive, however typically not by a lot. Even though this increased expense often comes with a lot of additional configuration options, some of the students in the group have limited experience working with cloud servers, wherefore we believed Digital Ocean would be a good choice, as it is free, and there are plenty of guides and good documentation.

\subsection{CI/CD: GitHub Actions}

For CI/CD we chose GitHub Actions. The reason is that we are already using the GitHub platform and that it is well documented. Furthermore, it is easy to configure secrets and refer to them from the \texttt{.yaml} file. Additionally, it collaborates nicely with Digital Ocean which hosts our servers.

\subsection{Database: SQLite to PostgreSQL}

We chose to use PostgreSQL when we had to start creating our Dockerfile. We looked for a Microsoft or another official Docker image from a good source (that is from a big, known company, that we can trust). But we could not find one and we saw that Helge used something he created himself. Thus, we had to choose among some of the databases that has been containerised. That were the first two requirements. The third requirement is that we need a relational database. This excluded well-known document databases like MongoDB. At the end we had two good options. Microsoft SQL and PostgreSQL.

Feature-wise they offer the same things. They are both good at handling concurrent requests and have transactions. They both offer the possibility to add indexes, and to have multiple servers act as a cluster in order to handle even more requests.

We chose the solution that we have had experience with before. Both in our previous course *Introduction to Database Systems*, and in previous projects done at ITU. We have had an abundance of new technologies throughout our times as students, so it felt almost relieving to not use a new one again.

\subsection{Monitoring: Grafana}

Looking for a tool to visualise the metrics of dotnet applications, and maybe other tools in our tech stack as well, required a versatile tool. That is, we needed to have a tool where we could search and visualise the current status of our application. Obviously, a lot of tools can handle this. For that reason it is nice to see that the tool OpenTelemetry is announced and widely used - both as a collector and exporter.

Thus, a requirement is that the monitoring system has to be able to integrate with OpenTelemetry in order to make it easier to change monitoring system if required.

Furthermore, it would be prefered to have an open source and free of charge product that is being maintained.

\subsection{Metrics Collector: Prometheus}

There has not been an elaborate thought process about the choice of Prometheus. It is open source and can collaborate with the OpenTelemetry exporter. Additionally, it is able to connect to the Docker metrics as well.

\subsection{Database Migration: Digital Ocean}

We have considered hosting a separate server on Digital Ocean, and using Digital Ocean's own database service. We considered Digital Ocean's options, due to the wide array of databases that they support. However, when researching about troubleshooting, and problems using this solution, we found very few answers from crowd sourced platforms such as StackOverflow, and a lot of guides from the Digital Ocean docs themselves.

\subsection{Logging Stack: Grafana + Loki + Promtail}

We believe that the amount of applications used can get very bloated. Therefore, we have decided to use Grafana-Loki, as we have already setup Grafana last week when implementing monitoring.

\subsection{Quality of Code Analysis: SonarCloud}

SonarCloud is a cloud-based code analysis service, which can be integrated into a GitHub repository, in order to analyse the code in said repository. We added this software as it was an exercise in the course.

\subsection{Linters: Pre-commit + Hadolint + Dotnet-format}
Pre-commit is a framework for managing pre-commit hooks. This can be used for automatically fixing code formatting and removing debug statements. We chose this tool as multiple actions can be integrated into one hook. As we were tasked with integrating three linters into the system.

Hadolint is an open source Dockerfile linter, that enforces best practices when building a Docker image. Due to the nature of the server setup, we have quite a few Dockerfiles. Therefore, we found it fitting to have a Dockerfile linter. From the alternative presented (\href{https://github.com/RedCoolBeans/dockerlint}{Dockerlinter}), Hadolint seems to be the more popular one as it is Haskell-based rather than Node.js-based.

Dotnet-format is a formatting tool, which is included in the .NET 6 SDK and onwards. It applies style preferences to a project which can be configured in an \texttt{.editorconfig} file.

\subsection{Configuration Management: Ansible}

We realise now that we need a tool that easily manages the desired state of our running VMs. Especially now that we are going to create more VMs and that we have to move our infrastructure before the 19th of April to another Digital Ocean account.

For that reason we need a simple way to have a configuration of our VMs defined that manages our provisioning. Our requirements are the following:

\begin{itemize}
    \item Simple setup
    \item Should be pushable i.e. as soon as we have changed the configuration, then it should push the desired state out to our servers
    \item Keeps track if specific portions of the provisioning scripts already have been run
    \item Does not require us to learn a new language
\end{itemize}

We have been looking at different players in the market. Especially we have been looking at the Cloud Native Computing Foundation (CNCF) and asking colleagues. Here are some of the products we have looked at:

\begin{itemize}
    \item Ansible
    \item Fabric - Not really a configuration management system but a “simple” push-based script system
    \item Chef
\end{itemize}

\subsubsection{Fabric}

Fabric seemed nice, however it lacked some of the basic things that we wanted to solve, such as the idempotence of the desired state of the VM. This we would have to script ourselves. Also we would have to add another language to our repo, that is Python, which would require us to setup our development container again to have both dotnet and Python. Our experience with that was that it was very hard to get right, and we finally have a setup now that works on all of our laptops. So we did not want to break that.

\subsubsection{Chef}

Chef is a configuration management system, just like Ansible as well. It provides a hybrid solution of pull- and push-based. That is, we push the configuration management changes to a central server and then the VMs will pull the changes from the server (probably after some interval).

As can be seen this also requires that the VMs has the Chef Client installed and configured to point at the Chef Server. This provides the nice feature that the Chef Server do not have to know which servers to configure. It is up to the Chef Clients to know where to pull the configurations from.

The downside of this setup is the complexity. It seems like a lot of things that needs to be taken care of, and we would have a load overhead by adding clients to our small web server.

In order to create a configuration we would have to create what they call cookbooks. This is written in Ruby as far as we can see. The nice thing about this is that there are testing frameworks implemented, which provides a way to be sure that the configuration works and configures as we intend it. However, while experimenting with Vagrant it has become obvious that our group does not have sufficient knowledge in Ruby and we would currently rather invest time in becoming better at using the observability tools that we have, rather than learning a new language.

From a security point of view Chef uses some of the protocols that we have already opened ports for. That is, it uses standard HTTP protocol to communicate between the server and nodes, and authorises through certificates.

\subsubsection{Ansible}

Some of the nice features in Ansible is that the setup seems fairly simple:

\begin{itemize}
    \item One server to distribute to a list of servers
    \item No additional clients to install
    \item No code for defining configurations, it uses a YAML to define the configuration of the VMs
\end{itemize}

The downside is that there is a central server that needs to have a list of the created VMs, which then needs to be maintained. Furthermore, from a security perspective, we would have to allow \texttt{ssh} and that the Ansible server would have to store the private \texttt{ssh} key to access all of the servers. We have a hard time trying to understand how we are supposed to push this to the Ansible server and then run it? Would that be another \texttt{ssh} session?

Looking a bit further into it is seems like the way to automate the configurations of the server can be done with a proprietary product called Ansible Automation, which requires a license. But some has been able to create a custom GitHub Action that can push these changes to the servers. This requires that the \texttt{ssh} port is open to the public, which it already is.

As with Chef, Ansible also has their own term for a configuration, i.e. a playbook. The term is from American sports, where each task in the playbook is a play that constitutes the playbook. Each play in the playbook is idempotent. Thus, it seems to provide simple idempotence, which is fine in most cases, but it some cases it may require some more advanced idempotence.

\subsubsection{Why Do We Choose Ansible?}

We chose Ansible because the setup seems simple. It provides the features that we need i.e. an easy setup that does not require a new client to be installed on our application servers. Configurations described in YAML and not in some other language like Ruby or Python.

\subsection{Adding End-to-End and Integration Tests Early}

Early in the project process we were given some integrations and end-to-end (E2E) tests for the frontend and Simulator API, respectively. This was beneficial when we had to develop our system to begin with, but we quickly realised that in order to be comfortable, and to ensure that our future changes in the software did not break our interface with the simulator, then we had to add them to our CI pipeline via the GitHub Actions workflows.

However, we realised again that having them in Python would make it harder for us to extend our tests or create more tests as we progress further in our maintenance cycle, wherefore we needed to refactor it to C\#.

\subsection{Infrastructure as Code (IaC): Pulumi}

We have lately been a bit frustrated at Vagrant. It seems like it lacks some collaborative features that makes it hard to work with. Furthermore, trying to make it collaborate with a configuration management tool, like Ansible, proved to be very challenging and time consuming considering that it should “just be plug and play”. We did not make that work, so to eliminate a pain that we currently have, we are looking towards encoding our infrastructure as code now. What is our requirements?

\begin{itemize}
    \item Should be easy to work with
    \begin{itemize}
        \item That is, we would prefer to keep the configuration in \textbf{YAML} or \textbf{C\#} as we do not want to increase complexity of learning a new language again
    \end{itemize}
    \item Can use it in a \textbf{GitHub Actions workflow} such that we can create changes in the Minitwit repository, create a pull-request, and then push changes in production when everything is working
    \item This is not a requirement, but it would be nice if we can use an **open source** tool.
    \item A tool which can work for multiple cloud providers, such that we can change provider if we want to. However, since we are at Digital Ocean (which we are pretty happy with so far) then we want them to support Digital Ocean.
\end{itemize}

We looked at the following options:
\begin{itemize}
    \item Vagrant - Is it still a good option or not?
    \item Terraform - As stated in a later lecture is something that we are going to look at
    \item Pulumi - A tool we stumbled upon in a Digital Ocean video
    \item OpenTofu
    \item Write code ourselves with a custom client to Digital Ocean
\end{itemize}

\subsubsection{Vagrant}

Is a very nice tool for creating infrastructure quickly. What we have experienced so far, however, is that it is hard to collaborate within. It saves metadata files on the local machine to keep some sort of state. It only pushes the creator's \texttt{ssh} key to the server, and not the other's \texttt{ssh} key, making us copy paste \texttt{ssh} keys to the server manually. Last but not least, it is written in a language that none of us have experience with (Ruby).
There is not any GitHub Actions for using Vagrant, which we think has something to do with the way that it handles its state. Looking at Vagrant's comparison between Vagrant and Terraform (which is also a tool made by hashicorp) the following is stated:

\say{Vagrant is a tool for managing development environments and Terraform is a tool for building infrastructure... and more features provided by Vagrant to ease development environment usage... Vagrant is for development environments.\cite{vagrant_vs_terraform}}


By this description, Hashicorp has not intended to build Vagrant for the scenario that we use it for today. That is, they do not and will not improve the tool in the direction that we intend to use it. Furthermore, it does not make sense for us to keep Vagrant to setup a local development environment because we already have Docker and Docker Compose that takes care of that.

\subsubsection{Terraform}

As we can see in the above section, Terraform is a product that is used for the specific functionality that we need. It can integrate with our GitHub workflow. It does so with Terraform Cloud to keep the state of the infrastructure. It can create infrastructure on multiple cloud vendors. It also has support for Digital Ocean.

One of the downsides is that they introduce their own language: HCL. This is something that we do not want to have. We want to limit the amount of languages that our repository contains to a minimum. Although we can see that they actually have what they call a CDK (Cloud Development Kit) for Terraform, however, we doubt that is their first priority to maintain. An upside to the CDK is that it can add testing to Terraform.

Terraform is not open source, which makes it harder to trust the tool. A quick search on the internet yields that others have forked a former repository of Terraform and created a Terraform look-a-like, called OpenTofu, which reveals that the community does not trust Hashicorp's development of the tool, now that they have transitioned from open source to a Business Source License.

\subsubsection{Pulumi}

Pulumi is similar to Terraform, in the way that it also provides a cloud solution to keep state of the infrastructure. It does not offer a new language to write the infrastructure, but instead offers SDKs in languages that we already know. Both Pulumi and Terraform follow what is called Desired State Infrastructure, meaning that they try to keep the infrastructure in the desired state that the code describes.

It contains support for CI / CD tools, so we can integrate it into our existing workflows. In contrast to Terraform, it is open source. Pulumi seems to be widely adopted and is known for being easy for developers to get started.

\subsubsection{OpenTofu}

OpenTofu is a forked version of Terraform. It keeps the structure in HCL. What makes this interesting is that the community has actively taken a decision to maintain this project in order for it to not be a BSL tool, distancing them from Hashicorp's decision with Terraform. This can be seen in the OpenTofu Manifesto\cite{opentofu_manifesto}. Another advantage to OpenTofu is that it has later been adopted in the Linux Foundation and for sure is here to stay.

As it is a copy of Terraform it has some of the same features and downsides. This can be seen by having their own custom language to describe the infrastructure called OpenTofu Language. OpenTofu has not done the same thing as Terraform in terms of creating a CDK. That means the only option is the OpenTofu Language.

Another downside is the fact that we are unsure whether there is support for Digital Ocean as a provider. We would think that there would be, but it is not clear and we will need to spend time looking into it.

\subsubsection{Write C\# Code With Digital Ocean Client}

This option may not be the best option. Compared to the others we will have a tight coupling to Digital Ocean. This will make it challenging for us to change provider in the future, if that is needed. This will also require us to keep an eye out for the changes to the general API and make adjustments to the custom client that we would have to make. We could also find a client that is made for us, written in C\#, but then we would have to find one that is being maintained.

We have realised that the maintenance of just written code for the infrastructure, becomes a project in itself. We would have to manage an abundance of other dependencies, which is taken care of from the other projects. Also we do not have any testing capabilities as the other tools have.

Furthermore, we will not benefit from things that others may have created. A large community for an open source tool allows for sharing solutions and code for that specific tool. We do not really have that option here, as we are going to create a complete new tool with this option.

\subsection{Scaling: Docker Swarm}

We need to add horizontal scaling to our system.

\textbf{Our requirements}

\begin{itemize}
    \item Open source
    \item Works well with Ansible/Docker/Pulumi.
\end{itemize}

\subsubsection{Docker Swarm}

\cite{ansible:dockerswarm}
\begin{itemize}
    \item Open source
    \item It is based on the Docker API, and therefore is **lightweight**, and **easy to use**.
    \item Docker Swarm allows for limited scaling, and is not recommended for big systems, so if Minitwit ever becomes a bigger service, this could potentially be a problem.
    \item It is easy to add new nodes to existing clusters as both manager and worker
    \item Compared to other tools like kubernetes
    \item Simple to install compared to Kubernetes
    \item Same common language that we are already using to navigate in the structure
    \item Limited guides on interactions between Docker Swarm and Ansible.
\end{itemize}

\subsubsection{Building Our Own Redundant Load Balancer}
\begin{itemize}
    \item Time-consuming to create
    \item Less information on how to build it
    \item Probably more challenging to add new nodes than for Docker Swarm
\end{itemize}

\subsubsection{Kubernetes}
\begin{itemize}
    \item Takes more planning to implement than Docker Swarm
    \item Has a quite steep learning curve
    \item It can sustain and manage large architectures and complex workloads
    \item Has a GUI
\end{itemize}

\subsection{Upgrade/Update Strategy: Rolling Updates}

Rolling updates is the default update strategy in Docker Swarm and therefore the one we have chosen so as to not make the implementation more complicated and “home-made” than it needs to be.
